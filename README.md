# MSCI641 Project (Large Language Model-LLM)
## Title: Enhancing Automated Medical Question-Answer Systems Using Fine-Tuned Large Language Models
#### Member
- **Name:** S M Taslim Uddin Raju and Abdul Raqeeb Mohammad 
- **Email:**  [smturaju@uwaterloo.ca](mailto:smturaju@uwaterloo.ca), [ar3moham@uwaterloo.ca](mailto:ar3moham@uwaterloo.ca)
- **Personal Email:** [taslimuddinraju7864@gmail.com](mailto:taslimuddinraju7864@gmail.com)   

### About the Project
Our project focuses on enhancing automated medical Question and Answer (Q&A) systems using fine-tuned Large Language Models (LLMs). We aim to improve the accuracy and efficiency of these systems in providing medical information. The project uses the MedQuAD dataset, sourced from reliable National Institutes of Health databases, which contains a wealth of medical questions and answers. We employed various LLM architectures, including decoder-only models like GPT-2 and Llama2, and encoder-decoder models such as Bloom and T5. These models were fine-tuned on the MedQuAD dataset to optimize their performance specifically for medical Q&A tasks. This approach allows us to identify the most effective model architecture for handling complex medical queries.

The process began with the meticulous preprocessing of the dataset using Natural Language Processing (NLP) techniques. This included steps like tokenization, lemmatization, and the removal of stopwords, which are essential for preparing the data for effective model training. We evaluated the performance of these models using metrics such as BLEU, METEOR, and ROUGE scores. These metrics helped assess the coherence and relevance of the responses generated by the models. Results indicated that encoder-decoder models, especially the T5 model, performed best, showcasing their superior ability to understand and generate accurate medical responses.

<div align="center">
<img src="Figure/System architecture_page.jpg" alt="Alt text" title="Hover text" height = "200" width="500"/>
<p><em>Architecture of automated question-answer systems using fine-tuned LLMs.</em></p>
</div>


## Project Structure

```plaintext
Project Root/
│   requirements.txt
│   README.md
│
├───Figure
│     BLEU4.pdf
|     ROUGE.jpg
|     System architecture.pdf
|     encoder&decoder.png
|     meteor.jpg
|     training_loss.jpg
|     validation_loss.jpg
│
├───LLM Models
|    Finetune-GPT2-Q&A.ipynb
|    Bloom.ipynb
|    Finetune-t5.ipynb
|    Llma2.ipynb
│
|───MedQuAD
    MedQuAD.csv
